{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f382da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import skimage.io as skio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a160a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in training data\n",
    "\n",
    "nose_train_pts = []\n",
    "nose_test_pts = []\n",
    "\n",
    "for j in range(1, 7): # viewpoint index\n",
    "    for i in range(1, 41):  # person index\n",
    "        if i == 8 or i == 12 or i == 14 or i == 15 or i == 22 or i == 30 or i == 35:\n",
    "            gender = 'f' # gender\n",
    "\n",
    "        else:\n",
    "            gender = 'm'\n",
    "        \n",
    "        root_dir = './imm_face_db/'\n",
    "\n",
    "        # load all facial keypoints/landmarks\n",
    "        file = open(root_dir + '{:02d}-{:d}{}.asf'.format(i,j,gender))\n",
    "        points = file.readlines()[16:74]\n",
    "        landmark = []\n",
    "\n",
    "        for point in points:\n",
    "            x,y = point.split('\\t')[2:4]\n",
    "            landmark.append([float(x), float(y)])\n",
    "\n",
    "        # the nose keypoint\n",
    "        nose_keypoint = np.array(landmark).astype('float32')[-6]\n",
    "        \n",
    "        if (i < 33):\n",
    "            nose_train_pts.append(nose_keypoint)\n",
    "        else:\n",
    "            nose_test_pts.append(nose_keypoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc406ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nose_train_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cff746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import torchvision.transforms\n",
    "\n",
    "train_imgs = []\n",
    "test_imgs = []\n",
    "for j in range(1, 7): # viewpoint index\n",
    "    for i in range(1, 41):  # person index\n",
    "        if i == 8 or i == 12 or i == 14 or i == 15 or i == 22 or i == 30 or i == 35:\n",
    "            gender = 'f' # gender\n",
    "\n",
    "        else:\n",
    "            gender = 'm'\n",
    "        \n",
    "        root_dir = './imm_face_db/'\n",
    "\n",
    "        # load all facial keypoints/landmarks\n",
    "        path = root_dir + '{:02d}-{:d}{}.jpg'.format(i,j,gender)\n",
    "\n",
    "        img = cv2.imread(str(path))\n",
    "\n",
    "        # You may need to convert the color.\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        normal_img = (gray_img/255) - 0.5\n",
    "        print(normal_img)\n",
    "        \n",
    "        resize_img = cv2.resize(normal_img, (80, 60), interpolation = cv2.INTER_LINEAR)\n",
    "#         im_pil = Image.fromarray(resize_img)\n",
    "        \n",
    "        if (i < 33):\n",
    "            train_imgs.append(resize_img)\n",
    "        else:\n",
    "            test_imgs.append(resize_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d064e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e6e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(3,4))\n",
    "plt.imshow(train_imgs[0],  cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c818d0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,4))\n",
    "implot = plt.imshow(train_imgs[2],  cmap = \"gray\")\n",
    "\n",
    "plt.scatter(nose_train_pts[2][0]*80, nose_train_pts[2][1]*60, c = 'r', s= 10)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a82650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=12, kernel_size=5) # 60 x 80 (H x W)\n",
    "        self.conv2 = nn.Conv2d(12, 16, 5) \n",
    "        self.conv3 = nn.Conv2d(16, 32, 5)\n",
    "#         self.conv4 = nn.Conv2d(64, 96, 5)\n",
    "#         self.conv4 = nn.Conv2d(96, 128, 5)\n",
    "\n",
    "#         self.max_pool2d = MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        self.fc1 = nn.Linear(in_features=768, out_features=100, bias=True) \n",
    "        self.fc2 = nn.Linear(in_features=100, out_features=2, bias=True)\n",
    "        self.flatten = nn.Flatten(start_dim=0, end_dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        print(x.shape)\n",
    "\n",
    "        x = F.relu(x)\n",
    "        print(x.shape)\n",
    "\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        print(x.shape)\n",
    "        x = self.conv2(x) # b c h w\n",
    "        print(x.shape)\n",
    "        x = F.relu(x)\n",
    "        print(x.shape)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        print(x.shape)\n",
    "        x = self.conv3(x)\n",
    "        print(x.shape)\n",
    "        x = F.relu(x)\n",
    "        print(x.shape)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        print(x.shape)\n",
    "\n",
    "        x = torch.flatten(x, 1) #  b c h w -> b c\n",
    "        print(x.shape)\n",
    "        x = self.fc1(x) # b c\n",
    "        print(x.shape)\n",
    "        x = F.relu(x)\n",
    "        print(x.shape)\n",
    "        x = self.fc2(x)\n",
    "        print(x.shape)\n",
    "        output = x\n",
    "        return output\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c34a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.image_paths = train_imgs\n",
    "        self.label_paths = nose_train_pts\n",
    "        # TODO: Iterate over files in dataset path, add image and label paths to lists.\n",
    "        # TODO: Randomly split into train and test partitions. Make sure the random split is the same each time.\n",
    "\n",
    "        assert len(self.image_paths) == len(self.label_paths)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_path = self.image_paths[index]\n",
    "        label_path = self.label_paths[index]\n",
    "        \n",
    "#         transform = transforms.Compose([transforms.PILToTensor()])\n",
    "        # transform = transforms.PILToTensor()\n",
    "        # Convert the PIL image to Torch tensor\n",
    "        print(label_path.shape)\n",
    "\n",
    "        image_path = image_path.reshape(image_path.shape[0], image_path.shape[1], 1)\n",
    "        image_path = image_path.transpose((2, 0, 1))\n",
    "\n",
    "        image_tensor = torch.tensor(image_path).to(torch.float32)\n",
    "        label_tensor = torch.tensor(label_path)\n",
    "        return image_tensor, label_tensor\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_paths)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d8758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.image_paths = test_imgs\n",
    "        self.label_paths = nose_test_pts\n",
    "        # TODO: Iterate over files in dataset path, add image and label paths to lists.\n",
    "        # TODO: Randomly split into train and test partitions. Make sure the random split is the same each time.\n",
    "\n",
    "        assert len(self.image_paths) == len(self.label_paths)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_path = self.image_paths[index]\n",
    "        label_path = self.label_paths[index]\n",
    "        \n",
    "#         transform = transforms.Compose([transforms.PILToTensor()])\n",
    "        # transform = transforms.PILToTensor()\n",
    "        # Convert the PIL image to Torch tensor\n",
    "        image_path = image_path.reshape(image_path.shape[0], image_path.shape[1], 1)\n",
    "        image_path = image_path.transpose((2, 0, 1))\n",
    "        print(image_path)\n",
    "        image_tensor = torch.tensor(image_path).to(torch.float32)\n",
    "        label_tensor = torch.tensor(label_path)\n",
    "        return image_tensor, label_tensor\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e3ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TrainDataset()\n",
    "# image_tensor, label_tensor = data\n",
    "len(train_data)\n",
    "# torch.max(image_tensor)\n",
    "# label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052c78c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TestDataset()\n",
    "# image_tensor, label_tensor = data\n",
    "len(test_data)\n",
    "# torch.max(image_tensor)\n",
    "# label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1102ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size = 1)\n",
    "test_loader = DataLoader(test_data, batch_size = 1)\n",
    "\n",
    "\n",
    "# for i, data in enumerate(train_loader):\n",
    "#     print(i , data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15386f87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2888c1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "criterion = nn.MSELoss()  # nn.MSELoss() nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.003)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9392c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = list(train_loader)[:152]\n",
    "\n",
    "validation_loader = list(train_loader)[152:193]\n",
    "\n",
    "print(len(validation_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5d082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the dataset multiple times\n",
    "num_epochs = 15\n",
    "loss_values = []\n",
    "valid_loss_values = []\n",
    "\n",
    "for epoch in range(num_epochs):  \n",
    "\n",
    "    running_losses = []\n",
    "    running_loss = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(training_loader):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        print(\"input\", inputs.shape)\n",
    "        print(\"labels\", labels.shape)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        outputs = net(inputs)\n",
    "        print(\"outut\", outputs.shape)\n",
    "\n",
    "        # compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update network parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_losses.append(loss.item())\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0) \n",
    "\n",
    "        if i % 50 == 0:    # print every 50 minibatches\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, sum(running_losses) / len(running_losses)))\n",
    "        \n",
    "    loss_values.append(running_loss / len(training_loader))\n",
    "    \n",
    "    \n",
    "    for i, data in enumerate(validation_loader):\n",
    "            inputs, labels = data\n",
    "            \n",
    "            # calculate outputs\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # print statistics\n",
    "            running_losses.append(loss.item())\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0) \n",
    "\n",
    "\n",
    "    valid_loss_values.append(running_loss / len(validation_loader))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "plt.plot(loss_values)\n",
    "plt.plot(valid_loss_values)\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.xlabel('Epoch #')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2925b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    # initialize a list to store our predictions\n",
    "    preds = []\n",
    "    \n",
    "    for i, data in enumerate(test_loader):\n",
    "        images, labels = data\n",
    "\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        preds.append(outputs)\n",
    "        \n",
    "print(preds)\n",
    "# plt.plot(loss_values)\n",
    "# plt.ylabel('MSE Loss')\n",
    "# plt.xlabel('Epoch #')  \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e986cc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,4))\n",
    "\n",
    "i = 1\n",
    "\n",
    "implot = plt.imshow(test_imgs[i],  cmap = \"gray\")\n",
    "\n",
    "plt.scatter(nose_test_pts[i][0]*80, nose_test_pts[i][1]*60, c = 'r', s= 10)\n",
    "plt.scatter(preds[i][0][0]*80, preds[i][0][1]*60, c = 'b', s= 10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02262afe",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d97a02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in training data\n",
    "\n",
    "face_train_pts = []\n",
    "face_test_pts = []\n",
    "\n",
    "for j in range(1, 7): # viewpoint index\n",
    "    for i in range(1, 41):  # person index\n",
    "        if i == 8 or i == 12 or i == 14 or i == 15 or i == 22 or i == 30 or i == 35:\n",
    "            gender = 'f' # gender\n",
    "\n",
    "        else:\n",
    "            gender = 'm'\n",
    "        \n",
    "        root_dir = './imm_face_db/'\n",
    "\n",
    "        # load all facial keypoints/landmarks\n",
    "        file = open(root_dir + '{:02d}-{:d}{}.asf'.format(i,j,gender))\n",
    "        points = file.readlines()[16:74]\n",
    "        landmark = []\n",
    "\n",
    "        for point in points:\n",
    "            x,y = point.split('\\t')[2:4]\n",
    "            landmark.append([float(x), float(y)])\n",
    "            \n",
    "            # the nose keypoint\n",
    "        face_keypoint = np.array(landmark).astype('float32')\n",
    "        \n",
    "        if (i < 33):\n",
    "            face_train_pts.append(face_keypoint)\n",
    "        else:\n",
    "            face_test_pts.append(face_keypoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdc27c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_train_pts[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84685781",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_train_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdd4596",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_train_imgs = []\n",
    "face_test_imgs = []\n",
    "for j in range(1, 7): # viewpoint index\n",
    "    for i in range(1, 41):  # person index\n",
    "        if i == 8 or i == 12 or i == 14 or i == 15 or i == 22 or i == 30 or i == 35:\n",
    "            gender = 'f' # gender\n",
    "\n",
    "        else:\n",
    "            gender = 'm'\n",
    "        \n",
    "        root_dir = './imm_face_db/'\n",
    "\n",
    "        # load all facial keypoints/landmarks\n",
    "        path = root_dir + '{:02d}-{:d}{}.jpg'.format(i,j,gender)\n",
    "\n",
    "        img = cv2.imread(str(path))\n",
    "\n",
    "        # You may need to convert the color.\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        normal_img = ((gray_img.astype(np.float32))/255) - 0.5\n",
    "        print(normal_img)\n",
    "        \n",
    "        resize_img = cv2.resize(normal_img, (160, 120), interpolation = cv2.INTER_LINEAR)\n",
    "        print(resize_img.shape)\n",
    "#         im_pil = Image.fromarray(resize_img)\n",
    "        \n",
    "        if (i < 33):\n",
    "            face_train_imgs.append(resize_img)\n",
    "        else:\n",
    "            face_test_imgs.append(resize_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fc80e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug.augmenters as iaa\n",
    "\n",
    "class FaceTrainDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.image_paths = face_train_imgs\n",
    "        self.label_paths = face_train_pts\n",
    "        # TODO: Iterate over files in dataset path, add image and label paths to lists.\n",
    "        # TODO: Randomly split into train and test partitions. Make sure the random split is the same each time.\n",
    "\n",
    "        assert len(self.image_paths) == len(self.label_paths)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_path = self.image_paths[index]\n",
    "        label_path = self.label_paths[index]\n",
    "        \n",
    "        # Convert the PIL image to Torch tensor\n",
    "\n",
    "#         image_path = image_path.astype(np.uint8)\n",
    "#         image_path = image_path.reshape(1 , image_path.shape[0], image_path.shape[1])\n",
    "        \n",
    "        seq = iaa.SomeOf(2, \n",
    "                [iaa.AdditiveGaussianNoise(scale=0.05*255),\n",
    "                iaa.Affine(translate_px={\"x\": (-10, 10)}),\n",
    "                iaa.Affine(rotate = (-15, 15)),\n",
    "                iaa.Fliplr(1),\n",
    "                iaa.Sharpen(alpha=0.5),\n",
    "                ])\n",
    "        \n",
    "        label_path = label_path.reshape((1, 58, 2))\n",
    "        images_aug, points_aug = seq(images=image_path, keypoints=label_path)\n",
    "\n",
    "        label_path = label_path.reshape((58, 2))\n",
    "# #         print(images_aug.shape)\n",
    "# #         print(points_aug)\n",
    "#         image_path = (images_aug/255) - 0.5\n",
    "#         image_tensor = torch.tensor(image_path)\n",
    "#         label_tensor = torch.tensor(points_aug)\n",
    "#         return image_tensor, label_tensor\n",
    "\n",
    "#         image_path = self.image_paths[index]\n",
    "#         label_path = self.label_paths[index]\n",
    "\n",
    "\n",
    "        image_path = image_path.reshape(image_path.shape[0], image_path.shape[1], 1)\n",
    "        image_path = image_path.transpose((2, 0, 1))\n",
    "\n",
    "        image_tensor = torch.tensor(image_path).to(torch.float32)\n",
    "        label_tensor = torch.tensor(label_path)\n",
    "        label_tensor = torch.reshape(label_tensor, (116, 1))\n",
    "        return image_tensor, label_tensor\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_paths)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f34060",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_train_data = FaceTrainDataset()\n",
    "# image_tensor, label_tensor = data\n",
    "len(face_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03eff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_train_loader = DataLoader(face_train_data, batch_size = 1)\n",
    "face_training_loader = list(face_train_loader)[:152]\n",
    "face_validation_loader = list(face_train_loader)[152:193]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b0f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(face_train_loader)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a9d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_test_data = FaceTestDataset()\n",
    "face_test_loader = DataLoader(face_test_data, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40a31ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,8))\n",
    "i = 1 #batch number #(1-24)\n",
    "j = 1 #face in batch (1-4)\n",
    "face = face_training_loader[i][0][j].numpy()\n",
    "print(face.shape)\n",
    "implot = plt.imshow(face.reshape(face.shape[1], face.shape[2], 1),  cmap = \"gray\")\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "for pt in face_training_loader[i][1][j].numpy():\n",
    "    for index in pt:\n",
    "        xs.append(index[0] * 160)\n",
    "        ys.append(index[1] * 120)\n",
    "        \n",
    "        \n",
    "plt.scatter(np.asarray(xs), np.asarray(ys), c = 'r', s= 6)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6f24d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FaceNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FaceNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5) # 60 x 80 (H x W)\n",
    "        self.conv2 = nn.Conv2d(16, 24, 5) \n",
    "        self.conv3 = nn.Conv2d(24, 32, 1)\n",
    "        self.conv4 = nn.Conv2d(32, 64, 1)\n",
    "        self.conv5 = nn.Conv2d(64, 128, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=1536, out_features=256, bias=True) \n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=58*2, bias=True)\n",
    "        self.flatten = nn.Flatten(start_dim=0, end_dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "#         print(x.shape)\n",
    "\n",
    "        x = F.relu(x)\n",
    "#         print(x.shape)\n",
    "\n",
    "        x = F.max_pool2d(x, 2)\n",
    "#         print(x.shape)\n",
    "        x = self.conv2(x) # b c h w\n",
    "#         print(\"conv2\", x.shape)\n",
    "        x = F.relu(x)\n",
    "#         print(x.shape)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "#         print(x.shape)\n",
    "        x = self.conv3(x)\n",
    "#         print(\"conv3\", x.shape)\n",
    "        x = F.relu(x)\n",
    "#         print(x.shape)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "#         print(\"last 1\" , x.shape)\n",
    "        x = self.conv4(x)\n",
    "#         print(\"conv4\", x.shape)\n",
    "        x = F.relu(x)\n",
    "#         print(x.shape)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "#         print(x.shape)\n",
    "        x = self.conv5(x)\n",
    "#         print(\"conv5\", x.shape)\n",
    "        x = F.relu(x)\n",
    "#         print(x.shape)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "#         print(\"last pool\", x.shape)\n",
    "\n",
    "        x = torch.flatten(x, 1) #  b c h w -> b c\n",
    "#         print(x.shape)\n",
    "        x = self.fc1(x) # b c\n",
    "#         print(x.shape)\n",
    "        x = F.relu(x)\n",
    "#         print(x.shape)\n",
    "        x = self.fc2(x)\n",
    "#         print(x.shape)\n",
    "        output = x\n",
    "        return output\n",
    "\n",
    "facenet = FaceNet()\n",
    "print(facenet)\n",
    "\n",
    "\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class CNN2(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(CNN2, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 12, 3)\n",
    "#         self.conv2 = nn.Conv2d(12, 20, 3)\n",
    "#         self.conv3 = nn.Conv2d(20, 32, 3)\n",
    "#         self.conv4 = nn.Conv2d(32, 40, 3)\n",
    "#         self.conv5 = nn.Conv2d(40, 60, 3)\n",
    "        \n",
    "#         self.fully_connected_1 = nn.Linear(180, 140)\n",
    "#         self.fully_connected_2 = nn.Linear(140, 116)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = F.max_pool2d(F.relu(self.conv1(x)), kernel_size=2)\n",
    "#         x = F.max_pool2d(F.relu(self.conv2(x)), kernel_size=2)\n",
    "#         x = F.max_pool2d(F.relu(self.conv3(x)), kernel_size=2)\n",
    "#         x = F.max_pool2d(F.relu(self.conv4(x)), kernel_size=2)\n",
    "#         x = F.max_pool2d(F.relu(self.conv5(x)), kernel_size=2)\n",
    "        \n",
    "#         x = torch.flatten(x, 1)\n",
    "        \n",
    "#         x = F.relu(self.fully_connected_1(x))\n",
    "#         x = self.fully_connected_2(x)\n",
    "    \n",
    "#         return x\n",
    "    \n",
    "# facenet = CNN2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0283a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# facenet = CNN2()\n",
    "# epochs = 25\n",
    "# size = (120, 160)\n",
    "# transform = transforms.Compose([Rescale(size), Augment(), ToTensor()])\n",
    "# train_dataset, test_dataset, dataloader_test, dataloader_test = getDataloaders(size, transform, 1)\n",
    "# train_losses, test_losses, prediction = run_epoch(facenet, epochs, nn.MSELoss(), dataloader_test, dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb14d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, _, _, dataloader_test = getDataloaders(size, transform, 1)\n",
    "# i = 0\n",
    "# for pred in prediction:\n",
    "#     pred = pred.reshape(58,2)\n",
    "#     image = torch.reshape(dataloader_test.dataset[i]['image'], (120,160))\n",
    "#     keypoints = torch.reshape(dataloader_test.dataset[i]['landmarks'],(58,2))\n",
    "#     plt.imshow(image, cmap='gray')\n",
    "#     plt.scatter(160*keypoints[:,0], 120*keypoints[:,1], color='green')\n",
    "#     plt.scatter(160*pred[:,0], 120*pred[:,1], color='red')\n",
    "#     plt.show()\n",
    "#     i+=1\n",
    "# epoch = [i for i in range(len(train_losses))]\n",
    "# plt.plot(epoch, train_losses)\n",
    "# plt.plot(epoch, test_losses)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2767f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "facenet = FaceNet()\n",
    "criterion = nn.MSELoss()  # nn.MSELoss() nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(facenet.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd66b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the dataset multiple times\n",
    "num_epochs = 25\n",
    "loss_values = []\n",
    "valid_loss_values = []\n",
    "\n",
    "for epoch in range(num_epochs):  \n",
    "\n",
    "    running_losses = []\n",
    "    running_loss = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    \n",
    "    facenet.train()\n",
    "    \n",
    "    for i, data in enumerate(face_training_loader):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "#         print(\"input\", inputs.shape)\n",
    "#         print(inputs.cpu().detach().numpy().shape)\n",
    "#         plt.imshow(inputs.cpu().detach().numpy()[0,0,:,:])\n",
    "#         pts = labels.cpu().detach().numpy()[0,:,:].reshape(58, 2)\n",
    "#         plt.scatter(160*pts[:,0], 120*pts[:,1])\n",
    "#         plt.show()\n",
    "#         print(\"labels\", labels.shape)\n",
    "        # 16,1,120,160\n",
    "        \n",
    "        # 16,58,2\n",
    "        \n",
    "#         inputs = inputs.float()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        facenet.zero_grad()\n",
    "        \n",
    "#         print(labels.shape)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = facenet(inputs)\n",
    "#         print(\"output\" , outputs.shape)\n",
    "        # 16,58,2\n",
    "        \n",
    "        # compute loss\n",
    "#         outputs = outputs.reshape(labels.shape[0], labels.shape[1], labels.shape[2])\n",
    "\n",
    "        loss = criterion(outputs, labels[:,:,0])\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update network parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_losses.append(loss.item())\n",
    "        print(loss)\n",
    "        \n",
    "        running_loss += loss * inputs.size(0) \n",
    "\n",
    "#         if i % 50 == 0:    # print every 50 minibatches\n",
    "#             print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, sum(running_losses) / len(running_losses)))\n",
    "#     print(running_loss)\n",
    "    loss_values.append(running_loss.detach().numpy() / len(face_training_loader))\n",
    "    \n",
    "    facenet.eval()\n",
    "    for i, data in enumerate(face_validation_loader):\n",
    "            inputs, labels = data\n",
    "            \n",
    "            # calculate outputs\n",
    "            outputs = facenet(inputs)\n",
    "#             print(inputs.size)\n",
    "\n",
    "            # compute loss\n",
    "            loss = criterion(outputs, labels[:,:,0])\n",
    "#             print(loss)\n",
    "            loss = loss.detach().numpy()\n",
    "\n",
    "            # print statistics\n",
    "            running_losses.append(loss.item())\n",
    "\n",
    "            running_loss += loss * inputs.size(0) \n",
    "\n",
    "#             if i % 50 == 0:    # print every 50 minibatches\n",
    "#                 print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, sum(running_losses) / len(running_losses)))\n",
    "\n",
    "\n",
    "    valid_loss_values.append(running_loss.detach().numpy() / len(face_validation_loader))\n",
    "    \n",
    "    \n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "plt.plot(loss_values)\n",
    "plt.plot(valid_loss_values)\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.xlabel('Epoch #')\n",
    "# plt.savefig(\"facepts_mse.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e2563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(face_test_loader):\n",
    "    inputs, labels = data\n",
    "    outputs = facenet(inputs)\n",
    "    plt.imshow(inputs.cpu().detach().numpy()[0,0,:,:], cmap='gray')\n",
    "    pts = outputs.cpu().detach().numpy().reshape(58, 2)\n",
    "    plt.scatter(160*pts[:,0], 120*pts[:,1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156601f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize learned filter\n",
    "weights = facenet.conv1.weight.cpu().detach().numpy() \n",
    "print(len(weights))\n",
    "# for i in range(len(weights)):\n",
    "plt.figure()\n",
    "plt.imshow(np.squeeze(weights[15]), cmap = \"gray\")\n",
    "# plt.savefig(\"conv1_filter15.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c0e42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    # initialize a list to store our predictions\n",
    "    preds = []\n",
    "    \n",
    "    for i, data in enumerate(face_test_loader):\n",
    "        images, labels = data\n",
    "        print(images.shape)\n",
    "        print(labels.shape)\n",
    "\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = facenet(images)\n",
    "        print(outputs.shape)\n",
    "        preds.append(outputs)\n",
    "        \n",
    "print(preds)\n",
    "# plt.plot(loss_values)\n",
    "# plt.ylabel('MSE Loss')\n",
    "# plt.xlabel('Epoch #')  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf01cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,4))\n",
    "\n",
    "i = 1\n",
    "\n",
    "implot = plt.imshow(face_test_loader[i].numpy(),  cmap = \"gray\")\n",
    "\n",
    "# plt.scatter(nose_test_pts[i][0]*80, nose_test_pts[i][1]*60, c = 'r', s= 10)\n",
    "plt.scatter(preds[i][0][0]*160, preds[i][0][1]*120, c = 'b', s= 10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,8))\n",
    "i = 1 #batch number #(1-24)\n",
    "j = 1 #face in batch (1-8)\n",
    "face = face_training_loader[i][0][j].numpy()\n",
    "print(face.shape)\n",
    "implot = plt.imshow(face.reshape(face.shape[1], face.shape[2], 1),  cmap = \"gray\")\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "for pt in face_training_loader[i][1][j].numpy():\n",
    "    for index in pt:\n",
    "        xs.append(index[0] * 160)\n",
    "        ys.append(index[1] * 120)\n",
    "        \n",
    "        \n",
    "plt.scatter(np.asarray(xs), np.asarray(ys), c = 'r', s= 6)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6045bfb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a278cf8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f91b9360d971e2a884d39e8b7cea545ca2403965f67a223f92979c8f3628e17"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
